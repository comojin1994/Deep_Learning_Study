{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_NN_with_numerical_diff.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODI74hE5OZPLhnyBDpzs78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comojin1994/Deep_Learning_Study/blob/master/3step_lecture/Training_NN_with_numerical_diff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXfjVPSmmFjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e48c730-153b-4459-cac1-aa8f19acc4de"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml_2uM6qmanF",
        "colab_type": "text"
      },
      "source": [
        "### Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYoCxthKmTyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.0001\n",
        "\n",
        "def _t(x):\n",
        "  return np.transpose(x)\n",
        "\n",
        "def _m(A, B):\n",
        "  return np.matmul(A, B)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def MSE(h, y):\n",
        "  return 1 / 2 * np.mean(np.square(h - y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j17DMRlnmxhv",
        "colab_type": "text"
      },
      "source": [
        "### Make Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzHSjp3DmxJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron:\n",
        "  def __init__(self, W, b, a):\n",
        "    # Model Parameter\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.a = a\n",
        "\n",
        "    # Gradients\n",
        "    self.dW = np.zeros_like(self.W)\n",
        "    self.db = np.zeros_like(self.b)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.a(_m(_t(self.W), x) + self.b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlYPoGLnnI-M",
        "colab_type": "text"
      },
      "source": [
        "### Make DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GriXjDK_nIok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DNN:\n",
        "  def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
        "    def init_var(i, o):\n",
        "      return np.random.normal(0., 0.01, (i, o)), np.zeros((o,))\n",
        "    \n",
        "    self.sequence = list()\n",
        "    # First hidden layer\n",
        "    W, b = init_var(num_input, num_neuron)\n",
        "    self.sequence.append(Neuron(W, b, activation))\n",
        "\n",
        "    # Hidden layers\n",
        "    for _ in range(hidden_depth - 1):\n",
        "      W, b = init_var(num_neuron, num_neuron)\n",
        "      self.sequence.append(Neuron(W, b, activation))\n",
        "\n",
        "    # Output layer\n",
        "    W, b = init_var(num_neuron, num_output)\n",
        "    self.sequence.append(Neuron(W, b, activation))\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.sequence:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "    \n",
        "  def calc_gradient(self, x, y, loss_func):\n",
        "    def get_new_sequence(layer_index, new_neuron):\n",
        "      new_sequence = list()\n",
        "      for i, layer in enumerate(self.sequence):\n",
        "        if i == layer_index:\n",
        "          new_sequence.append(new_neuron)\n",
        "        else:\n",
        "          new_sequence.append(layer)\n",
        "      return new_sequence\n",
        "    \n",
        "    def eval_sequence(x, sequence):\n",
        "      for layer in self.sequence:\n",
        "        x = layer(x)\n",
        "      return x\n",
        "\n",
        "    loss = loss_func(self(x), y)\n",
        "\n",
        "    for layer_id, layer in enumerate(self.sequence): # iterate layer\n",
        "      for w_i, w in enumerate(layer.W): # iterate W (row)\n",
        "        for w_j, ww in enumerate(w): # iterate W (col)\n",
        "          W = np.copy(layer.W)\n",
        "          W[w_i][w_j] = ww + epsilon\n",
        "\n",
        "          new_neuron = Neuron(W, layer.b, layer.a)\n",
        "          new_seq = get_new_sequence(layer_id, new_neuron)\n",
        "          h = eval_sequence(x, new_seq)\n",
        "\n",
        "          num_grad = (loss_func(h, y) - loss) / epsilon\n",
        "          layer.dW[w_i][w_j] = num_grad\n",
        "      \n",
        "      for b_i, bb in enumerate(layer.b):\n",
        "        b = np.copy(layer.b)\n",
        "        b[b_i] = bb + epsilon\n",
        "\n",
        "        new_neuron = Neuron(layer.W, b, layer.a)\n",
        "        new_seq = get_new_sequence(layer_id, new_neuron)\n",
        "        h = eval_sequence(x, new_seq)\n",
        "\n",
        "        num_grad = (loss_func(h, y) - loss) / epsilon\n",
        "        layer.db[b_i] = num_grad\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjS3qFrBt3r2",
        "colab_type": "text"
      },
      "source": [
        "### Define GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etT55ZrWt17v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
        "  loss = network.calc_gradient(x, y, loss_obj)\n",
        "  for layer in network.sequence:\n",
        "    layer.W += -alpha * layer.dW\n",
        "    layer.b += -alpha * layer.db\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kcx-Wc5uWS3",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb3Y0oyWuVhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bba4561-cf78-4aad-88b1-16c4f245bd8c"
      },
      "source": [
        "x = np.random.normal(0., 1., (10, ))\n",
        "y = np.random.normal(0., 1., (2, ))\n",
        "\n",
        "dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n",
        "\n",
        "t = time.time()\n",
        "for epoch in range(100):\n",
        "  loss = gradient_descent(dnn, x, y, MSE, 0.01)\n",
        "  print('Epoch {}, Test Loss: {}'.format(epoch, loss))\n",
        "print('{} seconds elapsed.'.format(time.time() - t))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Test Loss: 1.2468301254638356\n",
            "Epoch 1, Test Loss: 1.2468301254638356\n",
            "Epoch 2, Test Loss: 1.2468301254638356\n",
            "Epoch 3, Test Loss: 1.2468301254638356\n",
            "Epoch 4, Test Loss: 1.2468301254638356\n",
            "Epoch 5, Test Loss: 1.2468301254638356\n",
            "Epoch 6, Test Loss: 1.2468301254638356\n",
            "Epoch 7, Test Loss: 1.2468301254638356\n",
            "Epoch 8, Test Loss: 1.2468301254638356\n",
            "Epoch 9, Test Loss: 1.2468301254638356\n",
            "Epoch 10, Test Loss: 1.2468301254638356\n",
            "Epoch 11, Test Loss: 1.2468301254638356\n",
            "Epoch 12, Test Loss: 1.2468301254638356\n",
            "Epoch 13, Test Loss: 1.2468301254638356\n",
            "Epoch 14, Test Loss: 1.2468301254638356\n",
            "Epoch 15, Test Loss: 1.2468301254638356\n",
            "Epoch 16, Test Loss: 1.2468301254638356\n",
            "Epoch 17, Test Loss: 1.2468301254638356\n",
            "Epoch 18, Test Loss: 1.2468301254638356\n",
            "Epoch 19, Test Loss: 1.2468301254638356\n",
            "Epoch 20, Test Loss: 1.2468301254638356\n",
            "Epoch 21, Test Loss: 1.2468301254638356\n",
            "Epoch 22, Test Loss: 1.2468301254638356\n",
            "Epoch 23, Test Loss: 1.2468301254638356\n",
            "Epoch 24, Test Loss: 1.2468301254638356\n",
            "Epoch 25, Test Loss: 1.2468301254638356\n",
            "Epoch 26, Test Loss: 1.2468301254638356\n",
            "Epoch 27, Test Loss: 1.2468301254638356\n",
            "Epoch 28, Test Loss: 1.2468301254638356\n",
            "Epoch 29, Test Loss: 1.2468301254638356\n",
            "Epoch 30, Test Loss: 1.2468301254638356\n",
            "Epoch 31, Test Loss: 1.2468301254638356\n",
            "Epoch 32, Test Loss: 1.2468301254638356\n",
            "Epoch 33, Test Loss: 1.2468301254638356\n",
            "Epoch 34, Test Loss: 1.2468301254638356\n",
            "Epoch 35, Test Loss: 1.2468301254638356\n",
            "Epoch 36, Test Loss: 1.2468301254638356\n",
            "Epoch 37, Test Loss: 1.2468301254638356\n",
            "Epoch 38, Test Loss: 1.2468301254638356\n",
            "Epoch 39, Test Loss: 1.2468301254638356\n",
            "Epoch 40, Test Loss: 1.2468301254638356\n",
            "Epoch 41, Test Loss: 1.2468301254638356\n",
            "Epoch 42, Test Loss: 1.2468301254638356\n",
            "Epoch 43, Test Loss: 1.2468301254638356\n",
            "Epoch 44, Test Loss: 1.2468301254638356\n",
            "Epoch 45, Test Loss: 1.2468301254638356\n",
            "Epoch 46, Test Loss: 1.2468301254638356\n",
            "Epoch 47, Test Loss: 1.2468301254638356\n",
            "Epoch 48, Test Loss: 1.2468301254638356\n",
            "Epoch 49, Test Loss: 1.2468301254638356\n",
            "Epoch 50, Test Loss: 1.2468301254638356\n",
            "Epoch 51, Test Loss: 1.2468301254638356\n",
            "Epoch 52, Test Loss: 1.2468301254638356\n",
            "Epoch 53, Test Loss: 1.2468301254638356\n",
            "Epoch 54, Test Loss: 1.2468301254638356\n",
            "Epoch 55, Test Loss: 1.2468301254638356\n",
            "Epoch 56, Test Loss: 1.2468301254638356\n",
            "Epoch 57, Test Loss: 1.2468301254638356\n",
            "Epoch 58, Test Loss: 1.2468301254638356\n",
            "Epoch 59, Test Loss: 1.2468301254638356\n",
            "Epoch 60, Test Loss: 1.2468301254638356\n",
            "Epoch 61, Test Loss: 1.2468301254638356\n",
            "Epoch 62, Test Loss: 1.2468301254638356\n",
            "Epoch 63, Test Loss: 1.2468301254638356\n",
            "Epoch 64, Test Loss: 1.2468301254638356\n",
            "Epoch 65, Test Loss: 1.2468301254638356\n",
            "Epoch 66, Test Loss: 1.2468301254638356\n",
            "Epoch 67, Test Loss: 1.2468301254638356\n",
            "Epoch 68, Test Loss: 1.2468301254638356\n",
            "Epoch 69, Test Loss: 1.2468301254638356\n",
            "Epoch 70, Test Loss: 1.2468301254638356\n",
            "Epoch 71, Test Loss: 1.2468301254638356\n",
            "Epoch 72, Test Loss: 1.2468301254638356\n",
            "Epoch 73, Test Loss: 1.2468301254638356\n",
            "Epoch 74, Test Loss: 1.2468301254638356\n",
            "Epoch 75, Test Loss: 1.2468301254638356\n",
            "Epoch 76, Test Loss: 1.2468301254638356\n",
            "Epoch 77, Test Loss: 1.2468301254638356\n",
            "Epoch 78, Test Loss: 1.2468301254638356\n",
            "Epoch 79, Test Loss: 1.2468301254638356\n",
            "Epoch 80, Test Loss: 1.2468301254638356\n",
            "Epoch 81, Test Loss: 1.2468301254638356\n",
            "Epoch 82, Test Loss: 1.2468301254638356\n",
            "Epoch 83, Test Loss: 1.2468301254638356\n",
            "Epoch 84, Test Loss: 1.2468301254638356\n",
            "Epoch 85, Test Loss: 1.2468301254638356\n",
            "Epoch 86, Test Loss: 1.2468301254638356\n",
            "Epoch 87, Test Loss: 1.2468301254638356\n",
            "Epoch 88, Test Loss: 1.2468301254638356\n",
            "Epoch 89, Test Loss: 1.2468301254638356\n",
            "Epoch 90, Test Loss: 1.2468301254638356\n",
            "Epoch 91, Test Loss: 1.2468301254638356\n",
            "Epoch 92, Test Loss: 1.2468301254638356\n",
            "Epoch 93, Test Loss: 1.2468301254638356\n",
            "Epoch 94, Test Loss: 1.2468301254638356\n",
            "Epoch 95, Test Loss: 1.2468301254638356\n",
            "Epoch 96, Test Loss: 1.2468301254638356\n",
            "Epoch 97, Test Loss: 1.2468301254638356\n",
            "Epoch 98, Test Loss: 1.2468301254638356\n",
            "Epoch 99, Test Loss: 1.2468301254638356\n",
            "37.674245834350586 seconds elapsed.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
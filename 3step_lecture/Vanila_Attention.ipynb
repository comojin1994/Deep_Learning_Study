{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanila_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nmtV-Iai1gRp0Z7Z7d11GGUIAVcHLT4D",
      "authorship_tag": "ABX9TyMQLBMIp9nMs/Ybjh40flxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comojin1994/Deep_Learning_Study/blob/master/3step_lecture/Vanila_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4izAa71PKhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c04d3526-d3ac-4ef8-92b4-b4e7bd2d8d70"
      },
      "source": [
        "%cd ./drive/My\\ Drive/Deep_learning/Study/3Step"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Deep_learning/Study/3Step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSkFLv9ltKuy",
        "colab_type": "text"
      },
      "source": [
        "### Korea natural language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KjGeIams4ad",
        "colab_type": "code",
        "outputId": "dde6c7cf-8529-41c1-cb96-edf0eb04d0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 159kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.2)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/3c/1dbe5d6943b5c68e8df17c8b3a05db4725eadb5c7b7de437506aa3030701/JPype1-0.7.2-cp36-cp36m-manylinux1_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, beautifulsoup4, tweepy, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1SfRCGGEGeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14585105-9309-478d-914d-e8fbf24ed100"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuawOlj3sxSp",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD0phkzrs0b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "NUM_WORDS = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ost0nn-5tXoT",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiGllOE3tY03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
        "        ### return_state : 중간 과정을 출력할것인지 True이면 output state, hidden state, cell state 출력\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        x = self.emb(x)\n",
        "        H, h, c = self.lstm(x)\n",
        "        return H, h, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCXrjGIixA1s",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA0XaW9axCCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
        "        ### return_sequences : False일 경우 마지막 output 1개만 출력됨\n",
        "        ### return_sequences : True일 경우 모든 output 출력됨\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "        self.att = tf.keras.layers.Attention()\n",
        "        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        x, s0, c0, H = inputs\n",
        "        x = self.emb(x)\n",
        "        S, h, c = self.lstm(x, initial_state=[s0, c0])\n",
        "        \n",
        "        S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1)\n",
        "        A = self.att([S_, H])\n",
        "        y = tf.concat([S, A], axis=-1)\n",
        "        \n",
        "        return self.dense(y), h, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8S3dvAN9WPt",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN7_1gV-Ju8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "    def __init__(self, sos, eos):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.enc = Encoder()\n",
        "        self.dec = Decoder()\n",
        "        self.sos = sos\n",
        "        self.eos = eos\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        if training is True:\n",
        "            x, y = inputs\n",
        "            H, h, c = self.enc(x)\n",
        "            y, _, _ = self.dec((y, h, c, H))\n",
        "            return y\n",
        "        else:\n",
        "            x = inputs\n",
        "            H, h, c = self.enc(x)\n",
        "            \n",
        "            y = tf.convert_to_tensor(self.sos)\n",
        "            y = tf.reshape(y, (1, 1))\n",
        "\n",
        "            seq = tf.TensorArray(tf.int32, 64)\n",
        "\n",
        "            for idx in tf.range(64):\n",
        "                y, h, c = self.dec([y, h, c, H])\n",
        "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
        "                y = tf.reshape(y, (1, 1))\n",
        "                seq = seq.write(idx, y)\n",
        "\n",
        "                if y == self.eos:\n",
        "                    break\n",
        "\n",
        "            return tf.reshape(seq.stack(), (1, 64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70BG8Ff_Laj9",
        "colab_type": "text"
      },
      "source": [
        "### Define Training, Test loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9UOqMljLdnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
        "    output_labels = labels[:, 1:]\n",
        "    shifted_labels = labels[:, :-1]\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([inputs, shifted_labels], training=True)\n",
        "        loss = loss_object(output_labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(output_labels, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(model, inputs):\n",
        "    return model(inputs, training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaGX_SqdOWpE",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-Rtc1EOYax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### http://www.aihub.or.kr\n",
        "dataset_file = 'chatbot_data.csv'\n",
        "okt = Okt()\n",
        "\n",
        "with open(dataset_file, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    seq = [' '.join(okt.morphs(line)) for line in lines]\n",
        "\n",
        "questions = seq[::2]\n",
        "answers = ['\\t' + lines for lines in seq[1::2]]\n",
        "\n",
        "num_sample = len(questions)\n",
        "\n",
        "perm = list(range(num_sample))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(perm)\n",
        "\n",
        "train_q = list()\n",
        "train_a = list()\n",
        "test_q = list()\n",
        "test_a = list()\n",
        "\n",
        "for idx, qna in enumerate(zip(questions, answers)):\n",
        "    q, a = qna\n",
        "    if perm[idx] > num_sample//5:\n",
        "        train_q.append(q)\n",
        "        train_a.append(a)\n",
        "\n",
        "    else:\n",
        "        test_q.append(q)\n",
        "        test_a.append(a)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
        "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "tokenizer.fit_on_texts(train_q + train_a)\n",
        "\n",
        "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
        "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
        "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
        "                                                        maxlen=64,\n",
        "                                                        padding='pre',\n",
        "                                                        value=0)\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
        "                                                        maxlen=65,\n",
        "                                                        padding='post',\n",
        "                                                        value=0)\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
        "                                                       maxlen=64,\n",
        "                                                       padding='pre',\n",
        "                                                       value=0)\n",
        "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
        "                                                       maxlen=65,\n",
        "                                                       padding='post',\n",
        "                                                       value=0)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce49DqjKS7PW",
        "colab_type": "text"
      },
      "source": [
        "### Define Train env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avEpOm0YS_YB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model\n",
        "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
        "                eos=tokenizer.word_index['\\n'])\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define performance metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPuRNekbTtWO",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pRHTAu3T5xY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe64cb6c-68fd-4fe5-f177-def378e6cded"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    for seqs, labels in train_ds:\n",
        "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "\n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
        "    print(template.format(epoch + 1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result() * 100))\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 2.930478811264038, Accuracy: 84.375\n",
            "Epoch 2, Loss: 0.6041969656944275, Accuracy: 91.7567367553711\n",
            "Epoch 3, Loss: 0.5153856873512268, Accuracy: 91.87813568115234\n",
            "Epoch 4, Loss: 0.4968894422054291, Accuracy: 91.90554809570312\n",
            "Epoch 5, Loss: 0.48965027928352356, Accuracy: 91.93687438964844\n",
            "Epoch 6, Loss: 0.4868849217891693, Accuracy: 91.92121124267578\n",
            "Epoch 7, Loss: 0.47463688254356384, Accuracy: 92.01127624511719\n",
            "Epoch 8, Loss: 0.45996472239494324, Accuracy: 92.0817642211914\n",
            "Epoch 9, Loss: 0.45165082812309265, Accuracy: 92.20707702636719\n",
            "Epoch 10, Loss: 0.4340301752090454, Accuracy: 92.33631134033203\n",
            "Epoch 11, Loss: 0.415818989276886, Accuracy: 92.5399398803711\n",
            "Epoch 12, Loss: 0.40865370631217957, Accuracy: 92.90805053710938\n",
            "Epoch 13, Loss: 0.39594796299934387, Accuracy: 93.08427429199219\n",
            "Epoch 14, Loss: 0.38932546973228455, Accuracy: 93.24091339111328\n",
            "Epoch 15, Loss: 0.3833733797073364, Accuracy: 93.2996597290039\n",
            "Epoch 16, Loss: 0.3766595721244812, Accuracy: 93.3975601196289\n",
            "Epoch 17, Loss: 0.3688405156135559, Accuracy: 93.47196197509766\n",
            "Epoch 18, Loss: 0.3668338358402252, Accuracy: 93.44454956054688\n",
            "Epoch 19, Loss: 0.3554457128047943, Accuracy: 93.55419921875\n",
            "Epoch 20, Loss: 0.34929001331329346, Accuracy: 93.55028533935547\n",
            "Epoch 21, Loss: 0.35036802291870117, Accuracy: 93.59727478027344\n",
            "Epoch 22, Loss: 0.34018391370773315, Accuracy: 93.6442642211914\n",
            "Epoch 23, Loss: 0.33793145418167114, Accuracy: 93.64035034179688\n",
            "Epoch 24, Loss: 0.33580905199050903, Accuracy: 93.77741241455078\n",
            "Epoch 25, Loss: 0.33097776770591736, Accuracy: 93.83615112304688\n",
            "Epoch 26, Loss: 0.32537272572517395, Accuracy: 93.82440185546875\n",
            "Epoch 27, Loss: 0.3146216571331024, Accuracy: 93.86747741699219\n",
            "Epoch 28, Loss: 0.3175467848777771, Accuracy: 93.92622375488281\n",
            "Epoch 29, Loss: 0.3110271096229553, Accuracy: 93.91447448730469\n",
            "Epoch 30, Loss: 0.3055233657360077, Accuracy: 93.96537780761719\n",
            "Epoch 31, Loss: 0.29984691739082336, Accuracy: 93.94971466064453\n",
            "Epoch 32, Loss: 0.29780012369155884, Accuracy: 94.07111358642578\n",
            "Epoch 33, Loss: 0.29289510846138, Accuracy: 94.07894897460938\n",
            "Epoch 34, Loss: 0.2875344753265381, Accuracy: 94.07502746582031\n",
            "Epoch 35, Loss: 0.2870948016643524, Accuracy: 94.12593841552734\n",
            "Epoch 36, Loss: 0.27771809697151184, Accuracy: 94.1807632446289\n",
            "Epoch 37, Loss: 0.2726563811302185, Accuracy: 94.23558807373047\n",
            "Epoch 38, Loss: 0.27483323216438293, Accuracy: 94.25516510009766\n",
            "Epoch 39, Loss: 0.2659798860549927, Accuracy: 94.2864990234375\n",
            "Epoch 40, Loss: 0.2687518000602722, Accuracy: 94.33348846435547\n",
            "Epoch 41, Loss: 0.26073652505874634, Accuracy: 94.36872863769531\n",
            "Epoch 42, Loss: 0.2567446827888489, Accuracy: 94.435302734375\n",
            "Epoch 43, Loss: 0.2528589367866516, Accuracy: 94.45096588134766\n",
            "Epoch 44, Loss: 0.2468973696231842, Accuracy: 94.55670928955078\n",
            "Epoch 45, Loss: 0.2482145130634308, Accuracy: 94.50579071044922\n",
            "Epoch 46, Loss: 0.24061301350593567, Accuracy: 94.56062316894531\n",
            "Epoch 47, Loss: 0.2363736778497696, Accuracy: 94.63502502441406\n",
            "Epoch 48, Loss: 0.2363470196723938, Accuracy: 94.65460968017578\n",
            "Epoch 49, Loss: 0.23082059621810913, Accuracy: 94.72509765625\n",
            "Epoch 50, Loss: 0.22440288960933685, Accuracy: 94.80733489990234\n",
            "Epoch 51, Loss: 0.22252625226974487, Accuracy: 94.90131378173828\n",
            "Epoch 52, Loss: 0.21730753779411316, Accuracy: 95.00313568115234\n",
            "Epoch 53, Loss: 0.21257241070270538, Accuracy: 95.05403900146484\n",
            "Epoch 54, Loss: 0.20763738453388214, Accuracy: 95.15193939208984\n",
            "Epoch 55, Loss: 0.2026957869529724, Accuracy: 95.28508758544922\n",
            "Epoch 56, Loss: 0.1953185498714447, Accuracy: 95.43389892578125\n",
            "Epoch 57, Loss: 0.19278858602046967, Accuracy: 95.46131134033203\n",
            "Epoch 58, Loss: 0.18863160908222198, Accuracy: 95.64144897460938\n",
            "Epoch 59, Loss: 0.18392926454544067, Accuracy: 95.73151397705078\n",
            "Epoch 60, Loss: 0.17439696192741394, Accuracy: 95.98213958740234\n",
            "Epoch 61, Loss: 0.17552612721920013, Accuracy: 95.95081329345703\n",
            "Epoch 62, Loss: 0.16927751898765564, Accuracy: 96.10354614257812\n",
            "Epoch 63, Loss: 0.16239751875400543, Accuracy: 96.20144653320312\n",
            "Epoch 64, Loss: 0.15905117988586426, Accuracy: 96.40899658203125\n",
            "Epoch 65, Loss: 0.15215930342674255, Accuracy: 96.56563568115234\n",
            "Epoch 66, Loss: 0.1465369313955307, Accuracy: 96.67528533935547\n",
            "Epoch 67, Loss: 0.14322549104690552, Accuracy: 96.76143646240234\n",
            "Epoch 68, Loss: 0.13717299699783325, Accuracy: 96.92591094970703\n",
            "Epoch 69, Loss: 0.13338728249073029, Accuracy: 97.05122375488281\n",
            "Epoch 70, Loss: 0.12767750024795532, Accuracy: 97.16478729248047\n",
            "Epoch 71, Loss: 0.12344102561473846, Accuracy: 97.35275268554688\n",
            "Epoch 72, Loss: 0.11826662719249725, Accuracy: 97.31751251220703\n",
            "Epoch 73, Loss: 0.11223765462636948, Accuracy: 97.43890380859375\n",
            "Epoch 74, Loss: 0.1094018965959549, Accuracy: 97.63471221923828\n",
            "Epoch 75, Loss: 0.10422508418560028, Accuracy: 97.7913589477539\n",
            "Epoch 76, Loss: 0.10096946358680725, Accuracy: 97.83443450927734\n",
            "Epoch 77, Loss: 0.09521865099668503, Accuracy: 98.0067367553711\n",
            "Epoch 78, Loss: 0.0906708762049675, Accuracy: 98.13988494873047\n",
            "Epoch 79, Loss: 0.08716332167387009, Accuracy: 98.19470977783203\n",
            "Epoch 80, Loss: 0.08311775326728821, Accuracy: 98.25736236572266\n",
            "Epoch 81, Loss: 0.07932010293006897, Accuracy: 98.41400146484375\n",
            "Epoch 82, Loss: 0.07631772011518478, Accuracy: 98.48057556152344\n",
            "Epoch 83, Loss: 0.07213657349348068, Accuracy: 98.66854858398438\n",
            "Epoch 84, Loss: 0.0689319372177124, Accuracy: 98.7390365600586\n",
            "Epoch 85, Loss: 0.06685587018728256, Accuracy: 98.69596099853516\n",
            "Epoch 86, Loss: 0.06334047019481659, Accuracy: 98.81735229492188\n",
            "Epoch 87, Loss: 0.05904242768883705, Accuracy: 99.01707458496094\n",
            "Epoch 88, Loss: 0.057024937123060226, Accuracy: 98.98966217041016\n",
            "Epoch 89, Loss: 0.05262341350317001, Accuracy: 99.1619644165039\n",
            "Epoch 90, Loss: 0.05030073970556259, Accuracy: 99.23637390136719\n",
            "Epoch 91, Loss: 0.047211743891239166, Accuracy: 99.25203704833984\n",
            "Epoch 92, Loss: 0.044435858726501465, Accuracy: 99.34210968017578\n",
            "Epoch 93, Loss: 0.04239393770694733, Accuracy: 99.38127136230469\n",
            "Epoch 94, Loss: 0.03965551406145096, Accuracy: 99.45567321777344\n",
            "Epoch 95, Loss: 0.0389481745660305, Accuracy: 99.47525024414062\n",
            "Epoch 96, Loss: 0.0356094129383564, Accuracy: 99.55748748779297\n",
            "Epoch 97, Loss: 0.033338770270347595, Accuracy: 99.58881378173828\n",
            "Epoch 98, Loss: 0.03163958340883255, Accuracy: 99.61231231689453\n",
            "Epoch 99, Loss: 0.030178125947713852, Accuracy: 99.63972473144531\n",
            "Epoch 100, Loss: 0.028499241918325424, Accuracy: 99.67497253417969\n",
            "Epoch 101, Loss: 0.027004629373550415, Accuracy: 99.67888641357422\n",
            "Epoch 102, Loss: 0.025749415159225464, Accuracy: 99.71412658691406\n",
            "Epoch 103, Loss: 0.024085627868771553, Accuracy: 99.74153900146484\n",
            "Epoch 104, Loss: 0.022895246744155884, Accuracy: 99.78462219238281\n",
            "Epoch 105, Loss: 0.02197853848338127, Accuracy: 99.77287292480469\n",
            "Epoch 106, Loss: 0.020610837265849113, Accuracy: 99.80419921875\n",
            "Epoch 107, Loss: 0.01956080086529255, Accuracy: 99.83161163330078\n",
            "Epoch 108, Loss: 0.019386183470487595, Accuracy: 99.81986236572266\n",
            "Epoch 109, Loss: 0.01876809448003769, Accuracy: 99.83161163330078\n",
            "Epoch 110, Loss: 0.01756405085325241, Accuracy: 99.84727478027344\n",
            "Epoch 111, Loss: 0.016570599749684334, Accuracy: 99.84727478027344\n",
            "Epoch 112, Loss: 0.015696914866566658, Accuracy: 99.87468719482422\n",
            "Epoch 113, Loss: 0.014576883055269718, Accuracy: 99.902099609375\n",
            "Epoch 114, Loss: 0.013430457562208176, Accuracy: 99.8942642211914\n",
            "Epoch 115, Loss: 0.012723293155431747, Accuracy: 99.93342590332031\n",
            "Epoch 116, Loss: 0.011996070854365826, Accuracy: 99.92167663574219\n",
            "Epoch 117, Loss: 0.011632106266915798, Accuracy: 99.92951202392578\n",
            "Epoch 118, Loss: 0.011307121254503727, Accuracy: 99.92559814453125\n",
            "Epoch 119, Loss: 0.010814914479851723, Accuracy: 99.92167663574219\n",
            "Epoch 120, Loss: 0.010470823384821415, Accuracy: 99.93734741210938\n",
            "Epoch 121, Loss: 0.009827597998082638, Accuracy: 99.93734741210938\n",
            "Epoch 122, Loss: 0.008943576365709305, Accuracy: 99.9608383178711\n",
            "Epoch 123, Loss: 0.008769514039158821, Accuracy: 99.94908905029297\n",
            "Epoch 124, Loss: 0.008224550634622574, Accuracy: 99.94908905029297\n",
            "Epoch 125, Loss: 0.008103089407086372, Accuracy: 99.96475219726562\n",
            "Epoch 126, Loss: 0.00858553871512413, Accuracy: 99.94517517089844\n",
            "Epoch 127, Loss: 0.008050407283008099, Accuracy: 99.95301055908203\n",
            "Epoch 128, Loss: 0.007447258569300175, Accuracy: 99.95301055908203\n",
            "Epoch 129, Loss: 0.00677976431325078, Accuracy: 99.9608383178711\n",
            "Epoch 130, Loss: 0.006466430611908436, Accuracy: 99.96867370605469\n",
            "Epoch 131, Loss: 0.00614665262401104, Accuracy: 99.96867370605469\n",
            "Epoch 132, Loss: 0.005868499167263508, Accuracy: 99.97258758544922\n",
            "Epoch 133, Loss: 0.005613266956061125, Accuracy: 99.97258758544922\n",
            "Epoch 134, Loss: 0.005439441185444593, Accuracy: 99.96867370605469\n",
            "Epoch 135, Loss: 0.005433778278529644, Accuracy: 99.97258758544922\n",
            "Epoch 136, Loss: 0.005203080829232931, Accuracy: 99.97258758544922\n",
            "Epoch 137, Loss: 0.005118248984217644, Accuracy: 99.96867370605469\n",
            "Epoch 138, Loss: 0.004754633642733097, Accuracy: 99.97258758544922\n",
            "Epoch 139, Loss: 0.004561597481369972, Accuracy: 99.97650146484375\n",
            "Epoch 140, Loss: 0.004606205504387617, Accuracy: 99.97650146484375\n",
            "Epoch 141, Loss: 0.004522177390754223, Accuracy: 99.97650146484375\n",
            "Epoch 142, Loss: 0.005294993985444307, Accuracy: 99.9608383178711\n",
            "Epoch 143, Loss: 0.008286229334771633, Accuracy: 99.93342590332031\n",
            "Epoch 144, Loss: 0.0136719336733222, Accuracy: 99.82769775390625\n",
            "Epoch 145, Loss: 0.012788808904588223, Accuracy: 99.87077331542969\n",
            "Epoch 146, Loss: 0.009672307409346104, Accuracy: 99.90601348876953\n",
            "Epoch 147, Loss: 0.007810378447175026, Accuracy: 99.93342590332031\n",
            "Epoch 148, Loss: 0.006839488632977009, Accuracy: 99.9412612915039\n",
            "Epoch 149, Loss: 0.006953698117285967, Accuracy: 99.95301055908203\n",
            "Epoch 150, Loss: 0.005431302357465029, Accuracy: 99.96475219726562\n",
            "Epoch 151, Loss: 0.004901232197880745, Accuracy: 99.97650146484375\n",
            "Epoch 152, Loss: 0.004114095587283373, Accuracy: 99.97650146484375\n",
            "Epoch 153, Loss: 0.003724332433193922, Accuracy: 99.97650146484375\n",
            "Epoch 154, Loss: 0.003493808675557375, Accuracy: 99.96867370605469\n",
            "Epoch 155, Loss: 0.003200514940544963, Accuracy: 99.98433685302734\n",
            "Epoch 156, Loss: 0.0032002339139580727, Accuracy: 99.98042297363281\n",
            "Epoch 157, Loss: 0.0030383041594177485, Accuracy: 99.98433685302734\n",
            "Epoch 158, Loss: 0.0030393132474273443, Accuracy: 99.98042297363281\n",
            "Epoch 159, Loss: 0.0028566305991262197, Accuracy: 99.98433685302734\n",
            "Epoch 160, Loss: 0.002854167250916362, Accuracy: 99.97650146484375\n",
            "Epoch 161, Loss: 0.002686711959540844, Accuracy: 99.98042297363281\n",
            "Epoch 162, Loss: 0.002617837628349662, Accuracy: 99.98042297363281\n",
            "Epoch 163, Loss: 0.0025373271200805902, Accuracy: 99.98042297363281\n",
            "Epoch 164, Loss: 0.0024725869297981262, Accuracy: 99.98042297363281\n",
            "Epoch 165, Loss: 0.002469607163220644, Accuracy: 99.98042297363281\n",
            "Epoch 166, Loss: 0.0024588045198470354, Accuracy: 99.97258758544922\n",
            "Epoch 167, Loss: 0.0024501399602741003, Accuracy: 99.97650146484375\n",
            "Epoch 168, Loss: 0.0023895609192550182, Accuracy: 99.98433685302734\n",
            "Epoch 169, Loss: 0.0022849556989967823, Accuracy: 99.97258758544922\n",
            "Epoch 170, Loss: 0.0022575303446501493, Accuracy: 99.98042297363281\n",
            "Epoch 171, Loss: 0.002172798616811633, Accuracy: 99.97650146484375\n",
            "Epoch 172, Loss: 0.002352554351091385, Accuracy: 99.97258758544922\n",
            "Epoch 173, Loss: 0.0022731544449925423, Accuracy: 99.98042297363281\n",
            "Epoch 174, Loss: 0.0020766148809343576, Accuracy: 99.98433685302734\n",
            "Epoch 175, Loss: 0.0023114390205591917, Accuracy: 99.98042297363281\n",
            "Epoch 176, Loss: 0.0021824510768055916, Accuracy: 99.98042297363281\n",
            "Epoch 177, Loss: 0.0022215156350284815, Accuracy: 99.98042297363281\n",
            "Epoch 178, Loss: 0.0020023933611810207, Accuracy: 99.97650146484375\n",
            "Epoch 179, Loss: 0.0019470910774543881, Accuracy: 99.97650146484375\n",
            "Epoch 180, Loss: 0.00187794235534966, Accuracy: 99.98433685302734\n",
            "Epoch 181, Loss: 0.001865013618953526, Accuracy: 99.97650146484375\n",
            "Epoch 182, Loss: 0.0018900062423199415, Accuracy: 99.98433685302734\n",
            "Epoch 183, Loss: 0.001959208631888032, Accuracy: 99.97258758544922\n",
            "Epoch 184, Loss: 0.00181160110514611, Accuracy: 99.98042297363281\n",
            "Epoch 185, Loss: 0.0017834770260378718, Accuracy: 99.98433685302734\n",
            "Epoch 186, Loss: 0.0017255803104490042, Accuracy: 99.98433685302734\n",
            "Epoch 187, Loss: 0.001693513011559844, Accuracy: 99.98433685302734\n",
            "Epoch 188, Loss: 0.0017835639882832766, Accuracy: 99.98042297363281\n",
            "Epoch 189, Loss: 0.0016753737581893802, Accuracy: 99.97650146484375\n",
            "Epoch 190, Loss: 0.0017595905810594559, Accuracy: 99.98042297363281\n",
            "Epoch 191, Loss: 0.0016241185367107391, Accuracy: 99.98433685302734\n",
            "Epoch 192, Loss: 0.0015995410503819585, Accuracy: 99.98042297363281\n",
            "Epoch 193, Loss: 0.0015987275401130319, Accuracy: 99.97650146484375\n",
            "Epoch 194, Loss: 0.0016226156149059534, Accuracy: 99.97258758544922\n",
            "Epoch 195, Loss: 0.0016039666952565312, Accuracy: 99.97650146484375\n",
            "Epoch 196, Loss: 0.0015839615371078253, Accuracy: 99.98433685302734\n",
            "Epoch 197, Loss: 0.0016232726629823446, Accuracy: 99.97258758544922\n",
            "Epoch 198, Loss: 0.0016037304885685444, Accuracy: 99.97650146484375\n",
            "Epoch 199, Loss: 0.0015360602410510182, Accuracy: 99.98042297363281\n",
            "Epoch 200, Loss: 0.0015114943962544203, Accuracy: 99.98042297363281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYIurMGiUZ3l",
        "colab_type": "text"
      },
      "source": [
        "### Test loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_yPVW4xYBq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11c5ecdd-be32-47e8-b36e-a1c9208a418b"
      },
      "source": [
        "for test_seq, test_labels in test_ds:\n",
        "    predictions = test_step(model, test_seq)\n",
        "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
        "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
        "    texts = tokenizer.sequences_to_texts(predictions.numpy())\n",
        "    print('_')\n",
        "    print('q: ', test_text)\n",
        "    print('a: ', gt_text)\n",
        "    print('p: ', texts)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_\n",
            "q:  ['아이스 아메리카노 하나요 \\n']\n",
            "a:  ['\\t테이크아웃 하실 건가 요 \\n']\n",
            "p:  ['네 배달 비 3000원 입니다 \\n']\n",
            "_\n",
            "q:  ['여기 기프티콘 되죠 \\n']\n",
            "a:  ['\\t네 현금영수증 해드릴까 요 \\n']\n",
            "p:  ['네 배달 비 됩니다 \\n']\n",
            "_\n",
            "q:  ['진동 을 따로 주시나요 \\n']\n",
            "a:  ['\\t주 문 번호 로 드리겠습니다 \\n']\n",
            "p:  ['네 담아 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['커피 에 샷 추가 가능한가요 \\n']\n",
            "a:  ['\\t 네 가능합니다 \\n']\n",
            "p:  ['아뇨 매장 에서는 머그컵 만 사용 가능합니다 \\n']\n",
            "_\n",
            "q:  ['밀크 티 있나요 \\n']\n",
            "a:  ['\\t네 있습니다 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['밀크 티 종류 는 뭐 가 있어요 \\n']\n",
            "a:  ['\\t 루이보스 두 개 있습니다 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['카푸치노 는 로 주시 고 아메리카노 는 스몰 로 주시겠어요 \\n']\n",
            "a:  ['\\t 네 더 없으세요 \\n']\n",
            "p:  ['네 티 때 가 판매 \\n']\n",
            "_\n",
            "q:  ['조각 케이크 도 추가 해주시겠어요 \\n']\n",
            "a:  ['\\t 네 어떤 거 로 드릴 까요 \\n']\n",
            "p:  ['네 카운터 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
            "_\n",
            "q:  ['아메리카노 한잔 주세요 \\n']\n",
            "a:  ['\\t드시고 가시나요 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['커피 주문 할게요 \\n']\n",
            "a:  ['\\t네 어떤 걸 로 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['페퍼민트 티 하나 주세요 \\n']\n",
            "a:  ['\\t따뜻한 것 으로 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 랑 따뜻한 라떼 로 주세요 \\n']\n",
            "a:  ['\\t사이즈 는 어떻게 드릴 까요 \\n']\n",
            "p:  ['네 주문 가능합니다 \\n']\n",
            "_\n",
            "q:  ['언제 음료 가 \\n']\n",
            "a:  ['\\t10분 내 로 나갑니다 \\n']\n",
            "p:  ['네 배달 비 됩니다 \\n']\n",
            "_\n",
            "q:  ['캐리어 에 넣어 주실 수 있나요 \\n']\n",
            "a:  ['\\t아뇨 캐리어 는 2 개 부터 가능해요 \\n']\n",
            "p:  ['네 번호 입니다 \\n']\n",
            "_\n",
            "q:  ['아이스 로 주세요 \\n']\n",
            "a:  ['\\t지금 은 아이스 가 안됩니다 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['카페라떼 제일 사이즈 로 주세요 \\n']\n",
            "a:  ['\\t네 그럼 카페라떼 톨 사이즈 로 드리겠습니다 \\n']\n",
            "p:  ['여기 서 뭘 로 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['아니요 현금영수증 괜찮아요 \\n']\n",
            "a:  ['\\t진동 벨 울리면 찾으러 오시 면 됩니다 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 사이즈 업 해주세요 \\n']\n",
            "a:  ['\\t아이스 아메리카노 4000원 입니다 \\n']\n",
            "p:  ['네 주문 가능합니다 \\n']\n",
            "_\n",
            "q:  ['음료 한 잔 더 추가 되나요 \\n']\n",
            "a:  ['\\t네 말씀 하세요 \\n']\n",
            "p:  ['아뇨 알려주시면 적립 적립']\n",
            "_\n",
            "q:  ['네 바닐라 라테 따뜻한 걸 로 주세요 \\n']\n",
            "a:  ['\\t네 알겠습니다 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['자몽 에 이드 사이즈 업 되나요 \\n']\n",
            "a:  ['\\t네 자몽 에 이드 사이즈 업 가능해요 \\n']\n",
            "p:  ['14 500원 입니다 \\n']\n",
            "_\n",
            "q:  ['자몽 에 이드 랑 생크림 케이크 주세요 \\n']\n",
            "a:  ['\\t드시고 가시나요 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['네 화장실 은 어디 에요 \\n']\n",
            "a:  ['은 로 면 \\n']\n",
            "p:  ['네 주문 적립 해 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['이 쿠키 도 함께 주세요 \\n']\n",
            "a:  ['\\t네 쿠폰 찍어 드릴 까요 \\n']\n",
            "p:  ['네 있습니다 \\n']\n",
            "_\n",
            "q:  ['라테 에 우유 두 도 변경 가능한가요 \\n']\n",
            "a:  ['\\t네 라테 에 두유 로 변경 가능합니다 \\n']\n",
            "p:  ['네 카운터 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
            "_\n",
            "q:  ['네 몇 분 정도 걸릴까 요 \\n']\n",
            "a:  ['이 정도 거 예요 \\n']\n",
            "p:  ['네 번호 가게 마스코트 메뉴 에요 \\n']\n",
            "_\n",
            "q:  ['네 2 이 \\n']\n",
            "a:  ['\\t네 2 에 자리 \\n']\n",
            "p:  ['아뇨 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['카페인 이 음료 있나요 \\n']\n",
            "a:  ['\\t티 음료 와 스무디 에는 카페인 이 않습니다 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['딸기스무디 랑 키위 스무디 는 생 인가요 \\n']\n",
            "a:  ['는 키위 는 생 을 사용 하고 있습니다 \\n']\n",
            "p:  ['네 한 가능합니다 \\n']\n",
            "_\n",
            "q:  ['아니요 아메리카노 뜨거운 걸 로 주세요 \\n']\n",
            "a:  ['\\t드시고 가시나요 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['영수증 \\n']\n",
            "a:  ['\\t결제 완료 되셨고 영수증 여기 있습니다 \\n']\n",
            "p:  ['네 배달 비 3000원 입니다 \\n']\n",
            "_\n",
            "q:  ['커피 음료 것 뭐 가 있나요 \\n']\n",
            "a:  ['와 주스 있습니다 \\n']\n",
            "p:  ['네 담아 드립니다 \\n']\n",
            "_\n",
            "q:  ['주스 어떤 종류 있나요 \\n']\n",
            "a:  ['주스 주스 주스 가 있습니다 \\n']\n",
            "p:  ['네 500원 입니다 \\n']\n",
            "_\n",
            "q:  ['케이크 같이 주세요 \\n']\n",
            "a:  ['\\t네 알겠습니다 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['플랫 화이트 한 잔 주세요 \\n']\n",
            "a:  ['맛 과 고소한 맛 원두 중 에 어떤 걸 로 드릴 까요 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['얼마 인가요 \\n']\n",
            "a:  ['\\t플랫 화이트 4500원 입니다 \\n']\n",
            "p:  ['아니요 한 잔 로만 판매 하고 있습니다 \\n']\n",
            "_\n",
            "q:  ['치즈 케이크 는 지금 없나요 \\n']\n",
            "a:  ['\\t네 치즈케이크 는 지금 다 \\n']\n",
            "p:  ['네 초코 케이크 가 제일 잘나가요 \\n']\n",
            "_\n",
            "q:  ['다 하실 때 까지 안 되나요 \\n']\n",
            "a:  ['\\t네 도 해서 \\n']\n",
            "p:  ['네 카운터 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
            "_\n",
            "q:  ['화장실 이 어디 예요 \\n']\n",
            "a:  ['은 2 에 있습니다 \\n']\n",
            "p:  ['네 있습니다 \\n']\n",
            "_\n",
            "q:  ['을 \\n']\n",
            "a:  ['\\n']\n",
            "p:  ['네 배달 비 3000원 입니다 \\n']\n",
            "_\n",
            "q:  ['아아 에 얼음 많이 넣어주세요 \\n']\n",
            "a:  ['\\t네 다 되면 진동 벨 로 알려 드릴게요 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['아메리카노 기프티콘 사용 할 수 있나요 \\n']\n",
            "a:  ['\\t네 가능하세요 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['네 샌드위치 도 수 \\n']\n",
            "a:  ['\\n']\n",
            "p:  ['네 배달 비 3000원 입니다 \\n']\n",
            "_\n",
            "q:  ['포인트 랑 쿠폰 적립 해주세요 \\n']\n",
            "a:  ['\\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['세트 종류 가 어떤 게 \\n']\n",
            "a:  ['\\t이 쪽 에서 됩니다 \\n']\n",
            "p:  ['네 주문 가능합니다 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 되나요 \\n']\n",
            "a:  ['\\t네 가능합니다 \\n']\n",
            "p:  ['네 한 가능합니다 \\n']\n",
            "_\n",
            "q:  ['주문 할게요 \\n']\n",
            "a:  ['거 드릴 까요 \\n']\n",
            "p:  ['네 주문 적립 인데 케이크 주문 드리도록 돼요 \\n']\n",
            "_\n",
            "q:  ['번호 로 할게요 \\n']\n",
            "a:  ['는 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['주차 안 \\n']\n",
            "a:  ['\\t진동 벨 로 알려 드리겠습니다 \\n']\n",
            "p:  ['네 담아 드립니다 \\n']\n",
            "_\n",
            "q:  ['파나요 \\n']\n",
            "a:  ['는 계절 지금 은 \\n']\n",
            "p:  ['아니요 한 사이즈 가 판매 입니다 \\n']\n",
            "_\n",
            "q:  ['자몽 있나요 \\n']\n",
            "a:  ['\\t네 자몽 있습니다 \\n']\n",
            "p:  ['네 저희 가게 마스코트 메뉴 에요 \\n']\n",
            "_\n",
            "q:  ['그럼 자몽 차 한잔 주세요 \\n']\n",
            "a:  ['드릴 까요 \\n']\n",
            "p:  ['네 드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['그럼 감귤 라테 2 잔 이랑 쏙쏙 붕어빵 2 개 주세요 \\n']\n",
            "a:  ['\\t드시고 가시나요 \\n']\n",
            "p:  ['네 주문 적립 카드 가요 \\n']\n",
            "_\n",
            "q:  ['네 결제 는 카드 로 할게요 \\n']\n",
            "a:  ['\\t네 결제 완료 되었습니다 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 로 주세요 \\n']\n",
            "a:  ['\\t아이스 아메리카노 한잔 주문 도 와 드리겠습니다 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['아니요 아메리카노 는 아이스 로 주세요 \\n']\n",
            "a:  ['\\t네 음료 사이즈 는 어떤 걸 로 하시겠어요 \\n']\n",
            "p:  ['네 저희 가게 마스코트 메뉴 에요 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 에 얼음 조금 수 있나요 \\n']\n",
            "a:  ['\\t네 더 필요한 거 있으세요 \\n']\n",
            "p:  ['네 주문 가능합니다 \\n']\n",
            "_\n",
            "q:  ['네 블루베리 스무디 1 개 더 추가 로 주세요 \\n']\n",
            "a:  ['\\t드시고 가시나요 \\n']\n",
            "p:  ['네 뜨거운 로 드릴 까요 \\n']\n",
            "_\n",
            "q:  ['티 도 파나요 \\n']\n",
            "a:  ['\\t네 4500원 입니다 \\n']\n",
            "p:  ['네 한 가능합니다 \\n']\n",
            "_\n",
            "q:  ['랑 아메리카노 주세요 \\n']\n",
            "a:  ['\\t사이즈 선택 해주세요 \\n']\n",
            "p:  ['네 주문 안 입니다 \\n']\n",
            "_\n",
            "q:  ['저 아메리카노 에 샷 추가 해서 주세요 \\n']\n",
            "a:  ['\\t저희 샷 으로 샷 추가 해드릴까 요 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['주스 는 다른 건 없나요 \\n']\n",
            "a:  ['\\t그럼 이번 에 라테 추천 해드려요 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['멤버십 으로 사이 즈 업 해주세요 \\n']\n",
            "a:  ['\\t네 사이즈 업 도 와 드리겠습니다 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['카페라테 한잔 주세요 \\n']\n",
            "a:  ['\\t따뜻한 걸 로 드릴 까요 \\n']\n",
            "p:  ['여기 서 드시고 가나 요 가져가시나요 \\n']\n",
            "_\n",
            "q:  ['이 쿠키 는 뭐 예요 \\n']\n",
            "a:  ['\\t초코 칩 쿠키 입니다 \\n']\n",
            "p:  ['네 있습니다 \\n']\n",
            "_\n",
            "q:  ['많이 \\n']\n",
            "a:  ['은 많이 가 있고 다른 원두 들 보다 이 \\n']\n",
            "p:  ['네 배달 비 3000원 입니다 \\n']\n",
            "_\n",
            "q:  ['그리고 휘핑크림 은 에스프레소 크림 으로 올려주세요 \\n']\n",
            "a:  ['\\t결제 는 어떻게 해드릴까 요 \\n']\n",
            "p:  ['네 카운터 로 오시 면 테이크 아웃 잔 에 드리겠습니다 \\n']\n",
            "_\n",
            "q:  ['에스프레소 한 잔 하고 카페모카 두 잔 주세요 \\n']\n",
            "a:  ['\\t카페모카 사이즈 는 어떤 걸 드릴 까요 \\n']\n",
            "p:  ['여기 서 포인트 따뜻한 로 까요 \\n']\n",
            "_\n",
            "q:  ['할인 카드 있습니다 \\n']\n",
            "a:  ['\\t할인 되어 원 입니다 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['얼마 인가요 \\n']\n",
            "a:  ['원 입니다 \\n']\n",
            "p:  ['아니요 한 잔 로만 판매 하고 있습니다 \\n']\n",
            "_\n",
            "q:  ['그냥 따뜻한 걸 로 주시 고 따뜻한 아메리카노 도 한잔 더 주실 수 있나요 \\n']\n",
            "a:  ['가실 건가 요 \\n']\n",
            "p:  ['네 주문 가능합니다 \\n']\n",
            "_\n",
            "q:  ['네 갈 수 부탁드립니다 \\n']\n",
            "a:  ['있는 캐리어 에 담아 드리겠습니다 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['사이즈 업 되나요 \\n']\n",
            "a:  ['\\t네 \\n']\n",
            "p:  ['아니요 한 사이즈 로만 판매 하고 있습니다 \\n']\n",
            "_\n",
            "q:  ['포인트 사용 없이 적립 만 할게요 \\n']\n",
            "a:  ['\\t네 멤버십 카드 주시 면 도 와 드리겠습니다 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['그럼 와 아이스 아메리카노 로 할게요 \\n']\n",
            "a:  ['\\t더 필요하신 건 없나요 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['네 할인 적립 은 \\n']\n",
            "a:  ['\\t네 바코드 보여주세요 \\n']\n",
            "p:  ['네 배달 비 3000원 입니다 \\n']\n",
            "_\n",
            "q:  ['민트 초코 프라푸치노 주세요 \\n']\n",
            "a:  ['\\t휘핑 올려 드릴 까요 \\n']\n",
            "p:  ['네 더 필요한 거 없으신 가요 \\n']\n",
            "_\n",
            "q:  ['아이스 아메리카노 하나 랑 녹차 라테 주세요 \\n']\n",
            "a:  ['라테 도 아이스 로 드릴 까요 \\n']\n",
            "p:  ['네 주문 만 주문 만 주문 가능합니다 \\n']\n",
            "_\n",
            "q:  ['무료 인가요 \\n']\n",
            "a:  ['원 추가 입니다 \\n']\n",
            "p:  ['아니요 한 사이즈 가 판매 \\n']\n",
            "_\n",
            "q:  ['아메리카노 이 있는데 다른 걸 로 수 있을까요 \\n']\n",
            "a:  ['어떤 걸 로 바꿔 드릴 까요 \\n']\n",
            "p:  ['네 주문 음료 때 로 해주시면 가능합니다 \\n']\n",
            "_\n",
            "q:  ['이 카드 로 해주세요 \\n']\n",
            "a:  ['\\t음료 진동 벨 로 알려 드리겠습니다 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['멤버십 할인 되나요 \\n']\n",
            "a:  ['\\t네 멤버십 할인 하시면 10 할인 됩니다 \\n']\n",
            "p:  ['네 한 가능합니다 \\n']\n",
            "_\n",
            "q:  ['네 번호 로 할게요 \\n']\n",
            "a:  ['\\t네 에 번호 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['카페모카 는 따뜻한 거 로 주세요 \\n']\n",
            "a:  ['\\t카페모카 위 에 휘핑 올려 드릴 까요 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['세트 할인 되나요 \\n']\n",
            "a:  ['\\t네 까지 가능하십니다 \\n']\n",
            "p:  ['네 한 가능합니다 \\n']\n",
            "_\n",
            "q:  ['는 인데 요 \\n']\n",
            "a:  ['\\t네 원두 와 을 선택 하실 수 있습니다 \\n']\n",
            "p:  ['네 번호 가게 마스코트 메뉴 에요 \\n']\n",
            "_\n",
            "q:  ['카페라테 주세요 \\n']\n",
            "a:  ['\\t네 드시고 가시나요 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['바닐라 라테 를 두유 로 바꿔서 \\n']\n",
            "a:  ['\\t네 알겠습니다 \\n']\n",
            "p:  ['네 한 가능합니다 \\n']\n",
            "_\n",
            "q:  ['모카 라테 1 잔 주세요 \\n']\n",
            "a:  ['\\t휘핑크림 올려 드릴 까요 \\n']\n",
            "p:  ['여기 번호 드시고 가나 요 \\n']\n",
            "_\n",
            "q:  ['치즈 케이크 주문 할게요 \\n']\n",
            "a:  ['\\t네 아이스 아메리카노 두 잔 치즈 케이크 주문 받았습니다 \\n']\n",
            "p:  ['네 드시고 가시나요 \\n']\n",
            "_\n",
            "q:  ['여기 카페인 안 게 있나요 \\n']\n",
            "a:  ['\\t루이보스 랑 유자차 있어요 \\n']\n",
            "p:  ['네 담아 드립니다 \\n']\n",
            "_\n",
            "q:  ['유자차 차갑게 주문 가능한가요 \\n']\n",
            "a:  ['는 안되고 루이보스 는 아이스 로 가능하세요 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['포인트 적립 할게요 \\n']\n",
            "a:  ['\\t네 결제 \\n']\n",
            "p:  ['네 번호 찍어주세요 \\n']\n",
            "_\n",
            "q:  ['아메리카노 하나요 \\n']\n",
            "a:  ['\\t따뜻한 거 맞으세요 \\n']\n",
            "p:  ['네 저희 가게 마스코트 메뉴 에요 \\n']\n",
            "_\n",
            "q:  ['아니오 하나 \\n']\n",
            "a:  ['\\t네 여기 있습니다 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['아메리카노 2 잔 주세요 \\n']\n",
            "a:  ['\\t따뜻한 거 로 드릴 까요 \\n']\n",
            "p:  ['네 알겠습니다 \\n']\n",
            "_\n",
            "q:  ['그리고 도 1 잔 만 부탁드려요 \\n']\n",
            "a:  ['준비 해드리겠습니다 \\n']\n",
            "p:  ['크림 감귤 라테 가 제일 을 나가요 \\n']\n",
            "_\n",
            "q:  ['아니요 먹고 갈 거 예요 \\n']\n",
            "a:  ['는 몇 개 드릴 까요 \\n']\n",
            "p:  ['네 가능합니다 \\n']\n",
            "_\n",
            "q:  ['두 개 주세요 \\n']\n",
            "a:  ['거 면 유리잔 요 \\n']\n",
            "p:  ['네 주문 적립 입니다 \\n']\n",
            "_\n",
            "q:  ['시럽 은 어디 있나요 \\n']\n",
            "a:  ['\\t고객 님 시럽 은 에 면 있습니다 \\n']\n",
            "p:  ['네 카운터 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
            "_\n",
            "q:  ['네 감사합니다 \\n']\n",
            "a:  ['\\t진동 벨 로 알려 드릴게요 \\n']\n",
            "p:  ['아니요 한 사이즈 가 판매 입니다 \\n']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}